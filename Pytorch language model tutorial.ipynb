{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field,BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "from typing import Tuple\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize=\"spacy\",\n",
    "            tokenizer_language=\"it\",\n",
    "           init_token= '<sos>',\n",
    "           eos_token= '<eos>',\n",
    "           lower = True)\n",
    "\n",
    "TRG = Field(tokenize= \"spacy\",\n",
    "           tokenizer_language=\"it\",\n",
    "           init_token='<sos>',\n",
    "           eos_token='<eos>',\n",
    "           lower= True)\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
    "                                                    fields = (SRC, TRG))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data\t\t\t\t\t  tut3-model.pt\r\n",
      " it\t\t\t\t\t  Untitled1.ipynb\r\n",
      " itwiki-latest-pages-articles.xml\t  Untitled.ipynb\r\n",
      "'Pytorch language model tutorial.ipynb'   wikiextractor\r\n",
      " tut3-model-modified_attn.pt\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets =torchtext.datasets.TranslationDataset(path=\"data\",exts = ('/itaerr', '/itacor'),\n",
    "                                                    fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data= datasets.split([0.6,0.2,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7fb1c0e58850>>,\n",
       "            {'<unk>': 0,\n",
       "             '<pad>': 1,\n",
       "             '<sos>': 2,\n",
       "             '<eos>': 3,\n",
       "             'di': 4,\n",
       "             '_': 5,\n",
       "             'e': 6,\n",
       "             \"'\": 7,\n",
       "             'il': 8,\n",
       "             'la': 9,\n",
       "             'num': 10,\n",
       "             'in': 11,\n",
       "             'a': 12,\n",
       "             'che': 13,\n",
       "             'del': 14,\n",
       "             'un': 15,\n",
       "             'l': 16,\n",
       "             'è': 17,\n",
       "             'per': 18,\n",
       "             'della': 19,\n",
       "             'unk': 20,\n",
       "             'nel': 21,\n",
       "             'si': 22,\n",
       "             'le': 23,\n",
       "             'i': 24,\n",
       "             'una': 25,\n",
       "             'con': 26,\n",
       "             'da': 27,\n",
       "             'al': 28,\n",
       "             'dell': 29,\n",
       "             'non': 30,\n",
       "             'più': 31,\n",
       "             'dei': 32,\n",
       "             'alla': 33,\n",
       "             'come': 34,\n",
       "             'anche': 35,\n",
       "             'delle': 36,\n",
       "             'gli': 37,\n",
       "             'dal': 38,\n",
       "             'lo': 39,\n",
       "             'fu': 40,\n",
       "             'era': 41,\n",
       "             'sono': 42,\n",
       "             'o': 43,\n",
       "             'sua': 44,\n",
       "             'due': 45,\n",
       "             'ma': 46,\n",
       "             'nella': 47,\n",
       "             'all': 48,\n",
       "             'ad': 49,\n",
       "             'suo': 50,\n",
       "             'tra': 51,\n",
       "             'anni': 52,\n",
       "             'cui': 53,\n",
       "             'dalla': 54,\n",
       "             'd': 55,\n",
       "             'dopo': 56,\n",
       "             'prima': 57,\n",
       "             'ha': 58,\n",
       "             'se': 59,\n",
       "             'degli': 60,\n",
       "             'ed': 61,\n",
       "             'parte': 62,\n",
       "             'ai': 63,\n",
       "             'su': 64,\n",
       "             'stato': 65,\n",
       "             'questo': 66,\n",
       "             'uno': 67,\n",
       "             'nei': 68,\n",
       "             'marco': 69,\n",
       "             'sul': 70,\n",
       "             'essere': 71,\n",
       "             'ne': 72,\n",
       "             'suoi': 73,\n",
       "             'sia': 74,\n",
       "             'opere': 75,\n",
       "             'nell': 76,\n",
       "             'tre': 77,\n",
       "             'aveva': 78,\n",
       "             'solo': 79,\n",
       "             'alle': 80,\n",
       "             'venne': 81,\n",
       "             'vita': 82,\n",
       "             'stesso': 83,\n",
       "             'dall': 84,\n",
       "             'loro': 85,\n",
       "             'questa': 86,\n",
       "             'molto': 87,\n",
       "             'opera': 88,\n",
       "             'primo': 89,\n",
       "             'altri': 90,\n",
       "             'de': 91,\n",
       "             'to': 92,\n",
       "             'secondo': 93,\n",
       "             'gran': 94,\n",
       "             'periodo': 95,\n",
       "             're': 96,\n",
       "             'sulla': 97,\n",
       "             'mentre': 98,\n",
       "             'nelle': 99,\n",
       "             'sue': 100,\n",
       "             'dello': 101,\n",
       "             'secolo': 102,\n",
       "             'stati': 103,\n",
       "             'dai': 104,\n",
       "             'fine': 105,\n",
       "             'fino': 106,\n",
       "             'quale': 107,\n",
       "             'sempre': 108,\n",
       "             'no': 109,\n",
       "             'può': 110,\n",
       "             'beethoven': 111,\n",
       "             'campo': 112,\n",
       "             'città': 113,\n",
       "             'così': 114,\n",
       "             'poi': 115,\n",
       "             'quella': 116,\n",
       "             'anno': 117,\n",
       "             'quello': 118,\n",
       "             'questi': 119,\n",
       "             'ancora': 120,\n",
       "             'fra': 121,\n",
       "             'te': 122,\n",
       "             'dove': 123,\n",
       "             'letteratura': 124,\n",
       "             'alcuni': 125,\n",
       "             'grande': 126,\n",
       "             'italia': 127,\n",
       "             'lui': 128,\n",
       "             'durante': 129,\n",
       "             'ti': 130,\n",
       "             'volta': 131,\n",
       "             ' ': 132,\n",
       "             'epoca': 133,\n",
       "             'forma': 134,\n",
       "             'già': 135,\n",
       "             'nome': 136,\n",
       "             ',': 137,\n",
       "             'angolo': 138,\n",
       "             'inoltre': 139,\n",
       "             'negli': 140,\n",
       "             'schumacher': 141,\n",
       "             'dalle': 142,\n",
       "             'quali': 143,\n",
       "             'viene': 144,\n",
       "             'quando': 145,\n",
       "             'tempo': 146,\n",
       "             'erano': 147,\n",
       "             'seguito': 148,\n",
       "             'vi': 149,\n",
       "             'altre': 150,\n",
       "             'stata': 151,\n",
       "             'premio': 152,\n",
       "             'nuovo': 153,\n",
       "             'oltre': 154,\n",
       "             'sta': 155,\n",
       "             'verso': 156,\n",
       "             'jazz': 157,\n",
       "             'lucio': 158,\n",
       "             'però': 159,\n",
       "             'altro': 160,\n",
       "             'autore': 161,\n",
       "             'egli': 162,\n",
       "             'furono': 163,\n",
       "             'mondo': 164,\n",
       "             'tale': 165,\n",
       "             'angoli': 166,\n",
       "             'aver': 167,\n",
       "             'co': 168,\n",
       "             'li': 169,\n",
       "             'maggio': 170,\n",
       "             'numero': 171,\n",
       "             'ogni': 172,\n",
       "             'quindi': 173,\n",
       "             'senza': 174,\n",
       "             'termine': 175,\n",
       "             'lungo': 176,\n",
       "             'particolare': 177,\n",
       "             'proprio': 178,\n",
       "             'so': 179,\n",
       "             'stile': 180,\n",
       "             'agli': 181,\n",
       "             'hanno': 182,\n",
       "             'musica': 183,\n",
       "             'posto': 184,\n",
       "             'quanto': 185,\n",
       "             'titolo': 186,\n",
       "             'grazie': 187,\n",
       "             'guerra': 188,\n",
       "             'invece': 189,\n",
       "             'modo': 190,\n",
       "             'tutti': 191,\n",
       "             'mai': 192,\n",
       "             'morte': 193,\n",
       "             'punto': 194,\n",
       "             'stagione': 195,\n",
       "             'tutto': 196,\n",
       "             'uso': 197,\n",
       "             'base': 198,\n",
       "             'racconti': 199,\n",
       "             'ta': 200,\n",
       "             'tuttavia': 201,\n",
       "             'vero': 202,\n",
       "             'alcune': 203,\n",
       "             'allo': 204,\n",
       "             'caso': 205,\n",
       "             'circa': 206,\n",
       "             'fatto': 207,\n",
       "             'primi': 208,\n",
       "             'società': 209,\n",
       "             'soprattutto': 210,\n",
       "             'spesso': 211,\n",
       "             'vengono': 212,\n",
       "             'ci': 213,\n",
       "             'ciò': 214,\n",
       "             'infatti': 215,\n",
       "             'possono': 216,\n",
       "             'tedesco': 217,\n",
       "             'testi': 218,\n",
       "             'ce': 219,\n",
       "             'esempio': 220,\n",
       "             'giapponese': 221,\n",
       "             'meno': 222,\n",
       "             'oggi': 223,\n",
       "             'piano': 224,\n",
       "             'punti': 225,\n",
       "             'va': 226,\n",
       "             'akutagawa': 227,\n",
       "             'ni': 228,\n",
       "             'sotto': 229,\n",
       "             'attività': 230,\n",
       "             'causa': 231,\n",
       "             'ebbe': 232,\n",
       "             'famiglia': 233,\n",
       "             'genere': 234,\n",
       "             'giappone': 235,\n",
       "             'valore': 236,\n",
       "             'attraverso': 237,\n",
       "             'casanova': 238,\n",
       "             'contro': 239,\n",
       "             'corso': 240,\n",
       "             'm': 241,\n",
       "             'quel': 242,\n",
       "             'successo': 243,\n",
       "             'ca': 244,\n",
       "             'do': 245,\n",
       "             'padre': 246,\n",
       "             'quattro': 247,\n",
       "             'ri': 248,\n",
       "             'seconda': 249,\n",
       "             'sistema': 250,\n",
       "             'the': 251,\n",
       "             'uniti': 252,\n",
       "             'vennero': 253,\n",
       "             'apocrifi': 254,\n",
       "             'casa': 255,\n",
       "             'cioè': 256,\n",
       "             'gara': 257,\n",
       "             'quest': 258,\n",
       "             'amburgo': 259,\n",
       "             'an': 260,\n",
       "             'europa': 261,\n",
       "             'lingua': 262,\n",
       "             'molti': 263,\n",
       "             'origine': 264,\n",
       "             'paese': 265,\n",
       "             'trissino': 266,\n",
       "             'ultimo': 267,\n",
       "             'arte': 268,\n",
       "             'cultura': 269,\n",
       "             'dagli': 270,\n",
       "             'gruppo': 271,\n",
       "             'of': 272,\n",
       "             'perché': 273,\n",
       "             'produzione': 274,\n",
       "             'queste': 275,\n",
       "             'roma': 276,\n",
       "             'sa': 277,\n",
       "             'scuola': 278,\n",
       "             'sull': 279,\n",
       "             'fosse': 280,\n",
       "             'generale': 281,\n",
       "             'giovane': 282,\n",
       "             'imperatore': 283,\n",
       "             'nepal': 284,\n",
       "             'principali': 285,\n",
       "             'romanzo': 286,\n",
       "             'sarà': 287,\n",
       "             'storia': 288,\n",
       "             'sud': 289,\n",
       "             'antonino': 290,\n",
       "             'avrebbe': 291,\n",
       "             'comunque': 292,\n",
       "             'dato': 293,\n",
       "             'diversi': 294,\n",
       "             'equazioni': 295,\n",
       "             'età': 296,\n",
       "             'everest': 297,\n",
       "             'film': 298,\n",
       "             'giorno': 299,\n",
       "             'legge': 300,\n",
       "             'liuto': 301,\n",
       "             'movimento': 302,\n",
       "             'nuova': 303,\n",
       "             'prime': 304,\n",
       "             'que': 305,\n",
       "             'quelli': 306,\n",
       "             'sembra': 307,\n",
       "             'cima': 308,\n",
       "             'comune': 309,\n",
       "             'fa': 310,\n",
       "             'figlio': 311,\n",
       "             'grandi': 312,\n",
       "             'ii': 313,\n",
       "             'lavoro': 314,\n",
       "             'né': 315,\n",
       "             'persone': 316,\n",
       "             'propria': 317,\n",
       "             'senato': 318,\n",
       "             'sui': 319,\n",
       "             'sulle': 320,\n",
       "             'unione': 321,\n",
       "             'via': 322,\n",
       "             'buzzati': 323,\n",
       "             'diverse': 324,\n",
       "             'formula': 325,\n",
       "             'francese': 326,\n",
       "             'francia': 327,\n",
       "             'gare': 328,\n",
       "             'molte': 329,\n",
       "             'nonostante': 330,\n",
       "             'politica': 331,\n",
       "             'ra': 332,\n",
       "             'scrittore': 333,\n",
       "             'serie': 334,\n",
       "             'strumento': 335,\n",
       "             'autostrade': 336,\n",
       "             'cuba': 337,\n",
       "             'fece': 338,\n",
       "             'ferrari': 339,\n",
       "             'governo': 340,\n",
       "             'inizio': 341,\n",
       "             'libro': 342,\n",
       "             'livello': 343,\n",
       "             'maggiore': 344,\n",
       "             'mondiale': 345,\n",
       "             'new': 346,\n",
       "             'poco': 347,\n",
       "             'possibile': 348,\n",
       "             'pre': 349,\n",
       "             'tali': 350,\n",
       "             'tipo': 351,\n",
       "             'tutte': 352,\n",
       "             'volte': 353,\n",
       "             'autori': 354,\n",
       "             'capitale': 355,\n",
       "             'chi': 356,\n",
       "             'col': 357,\n",
       "             'crisi': 358,\n",
       "             'germania': 359,\n",
       "             'grado': 360,\n",
       "             'interno': 361,\n",
       "             'musicale': 362,\n",
       "             'nello': 363,\n",
       "             'presso': 364,\n",
       "             'quasi': 365,\n",
       "             'sarebbe': 366,\n",
       "             'scritti': 367,\n",
       "             'stessa': 368,\n",
       "             'strada': 369,\n",
       "             'tanto': 370,\n",
       "             'totale': 371,\n",
       "             'tradizione': 372,\n",
       "             'trova': 373,\n",
       "             'varie': 374,\n",
       "             'carica': 375,\n",
       "             'cinese': 376,\n",
       "             'dallo': 377,\n",
       "             'deve': 378,\n",
       "             'divenne': 379,\n",
       "             'esse': 380,\n",
       "             'esso': 381,\n",
       "             'fare': 382,\n",
       "             'letterario': 383,\n",
       "             'maggior': 384,\n",
       "             'morì': 385,\n",
       "             'ormai': 386,\n",
       "             'ozono': 387,\n",
       "             'partire': 388,\n",
       "             'presenza': 389,\n",
       "             'pri': 390,\n",
       "             'probabilmente': 391,\n",
       "             's': 392,\n",
       "             'sei': 393,\n",
       "             'van': 394,\n",
       "             'art': 395,\n",
       "             'aurelio': 396,\n",
       "             'giorni': 397,\n",
       "             'importante': 398,\n",
       "             'insieme': 399,\n",
       "             'interesse': 400,\n",
       "             'luglio': 401,\n",
       "             'luogo': 402,\n",
       "             'madre': 403,\n",
       "             'mare': 404,\n",
       "             'metri': 405,\n",
       "             'missili': 406,\n",
       "             'monte': 407,\n",
       "             'presenti': 408,\n",
       "             'pubblica': 409,\n",
       "             'quelle': 410,\n",
       "             'rispetto': 411,\n",
       "             'riuscì': 412,\n",
       "             'scrisse': 413,\n",
       "             'sviluppo': 414,\n",
       "             'tasso': 415,\n",
       "             'terzo': 416,\n",
       "             'tratta': 417,\n",
       "             'ultima': 418,\n",
       "             'za': 419,\n",
       "             'allora': 420,\n",
       "             'ben': 421,\n",
       "             'breve': 422,\n",
       "             'chiesa': 423,\n",
       "             'considerato': 424,\n",
       "             'essi': 425,\n",
       "             'far': 426,\n",
       "             'fase': 427,\n",
       "             'figura': 428,\n",
       "             'forte': 429,\n",
       "             'gennaio': 430,\n",
       "             'lettera': 431,\n",
       "             'me': 432,\n",
       "             'metà': 433,\n",
       "             'misura': 434,\n",
       "             'numerosi': 435,\n",
       "             'nuovi': 436,\n",
       "             'paesi': 437,\n",
       "             'pane': 438,\n",
       "             'pilota': 439,\n",
       "             'realtà': 440,\n",
       "             'rete': 441,\n",
       "             'risulta': 442,\n",
       "             'soli': 443,\n",
       "             'studi': 444,\n",
       "             'studio': 445,\n",
       "             'teatro': 446,\n",
       "             'testa': 447,\n",
       "             'tokyo': 448,\n",
       "             'tramite': 449,\n",
       "             'tutta': 450,\n",
       "             'vangeli': 451,\n",
       "             'vetta': 452,\n",
       "             'zione': 453,\n",
       "             'ambito': 454,\n",
       "             'campi': 455,\n",
       "             'centro': 456,\n",
       "             'cina': 457,\n",
       "             'codice': 458,\n",
       "             'condizioni': 459,\n",
       "             'essa': 460,\n",
       "             'forza': 461,\n",
       "             'giro': 462,\n",
       "             'impero': 463,\n",
       "             'influenza': 464,\n",
       "             'inglese': 465,\n",
       "             'kennedy': 466,\n",
       "             'na': 467,\n",
       "             'occasione': 468,\n",
       "             'ora': 469,\n",
       "             'ottobre': 470,\n",
       "             'parti': 471,\n",
       "             'pe': 472,\n",
       "             'personaggi': 473,\n",
       "             'pio': 474,\n",
       "             'po': 475,\n",
       "             'poesia': 476,\n",
       "             'pop': 477,\n",
       "             'pro': 478,\n",
       "             'prodotto': 479,\n",
       "             'pubblicato': 480,\n",
       "             'quota': 481,\n",
       "             'romanzi': 482,\n",
       "             'scritto': 483,\n",
       "             'sherpa': 484,\n",
       "             'sinfonia': 485,\n",
       "             'sovietica': 486,\n",
       "             'sovietici': 487,\n",
       "             'tema': 488,\n",
       "             'tro': 489,\n",
       "             'utilizzo': 490,\n",
       "             'vuoto': 491,\n",
       "             'acqua': 492,\n",
       "             'alagna': 493,\n",
       "             'alta': 494,\n",
       "             'bilancio': 495,\n",
       "             'cambio': 496,\n",
       "             'cariche': 497,\n",
       "             'cinque': 498,\n",
       "             'corrente': 499,\n",
       "             'cura': 500,\n",
       "             'data': 501,\n",
       "             'detto': 502,\n",
       "             'dicembre': 503,\n",
       "             'famoso': 504,\n",
       "             'forse': 505,\n",
       "             'fratello': 506,\n",
       "             'generi': 507,\n",
       "             'importanti': 508,\n",
       "             'infine': 509,\n",
       "             'intera': 510,\n",
       "             'libri': 511,\n",
       "             'nascita': 512,\n",
       "             'numerose': 513,\n",
       "             'operazione': 514,\n",
       "             'oro': 515,\n",
       "             'piuttosto': 516,\n",
       "             'poetica': 517,\n",
       "             'portò': 518,\n",
       "             'precedenti': 519,\n",
       "             'pubblico': 520,\n",
       "             'racconto': 521,\n",
       "             'ricerca': 522,\n",
       "             'ruolo': 523,\n",
       "             'san': 524,\n",
       "             'senso': 525,\n",
       "             'stessi': 526,\n",
       "             'sto': 527,\n",
       "             'subito': 528,\n",
       "             'superiore': 529,\n",
       "             'sé': 530,\n",
       "             'testamento': 531,\n",
       "             'ultimi': 532,\n",
       "             'uomo': 533,\n",
       "             'valle': 534,\n",
       "             'vangelo': 535,\n",
       "             'vinse': 536,\n",
       "             'vista': 537,\n",
       "             'vittoria': 538,\n",
       "             'almeno': 539,\n",
       "             'altra': 540,\n",
       "             'amore': 541,\n",
       "             'ampiezza': 542,\n",
       "             'antica': 543,\n",
       "             'azioni': 544,\n",
       "             'banconote': 545,\n",
       "             'castello': 546,\n",
       "             'classica': 547,\n",
       "             'diventa': 548,\n",
       "             'edo': 549,\n",
       "             'fiume': 550,\n",
       "             'forme': 551,\n",
       "             'gesù': 552,\n",
       "             'imperiale': 553,\n",
       "             'inizia': 554,\n",
       "             'italiana': 555,\n",
       "             'letteraria': 556,\n",
       "             'manson': 557,\n",
       "             'marzo': 558,\n",
       "             'mera': 559,\n",
       "             'milioni': 560,\n",
       "             'mo': 561,\n",
       "             'musashi': 562,\n",
       "             'nato': 563,\n",
       "             'nazionale': 564,\n",
       "             'nepalese': 565,\n",
       "             'nota': 566,\n",
       "             'ore': 567,\n",
       "             'par': 568,\n",
       "             'pari': 569,\n",
       "             'pochi': 570,\n",
       "             'popolare': 571,\n",
       "             'poteva': 572,\n",
       "             'pratica': 573,\n",
       "             'proprie': 574,\n",
       "             'pur': 575,\n",
       "             'qui': 576,\n",
       "             'seguenti': 577,\n",
       "             'situazione': 578,\n",
       "             'spagna': 579,\n",
       "             'spazio': 580,\n",
       "             'spedizione': 581,\n",
       "             'statunitense': 582,\n",
       "             'tardi': 583,\n",
       "             'adriano': 584,\n",
       "             'antico': 585,\n",
       "             'aprile': 586,\n",
       "             'area': 587,\n",
       "             'capo': 588,\n",
       "             'classifica': 589,\n",
       "             'composta': 590,\n",
       "             'corde': 591,\n",
       "             'corte': 592,\n",
       "             'dazai': 593,\n",
       "             'dedicata': 594,\n",
       "             'devono': 595,\n",
       "             'elementi': 596,\n",
       "             'esperienze': 597,\n",
       "             'eventi': 598,\n",
       "             'fatti': 599,\n",
       "             'fi': 600,\n",
       "             'finale': 601,\n",
       "             'importanza': 602,\n",
       "             'incontro': 603,\n",
       "             'india': 604,\n",
       "             'intorno': 605,\n",
       "             'italiano': 606,\n",
       "             'john': 607,\n",
       "             'lavori': 608,\n",
       "             'libertà': 609,\n",
       "             'locali': 610,\n",
       "             'località': 611,\n",
       "             'maggiori': 612,\n",
       "             'maxwell': 613,\n",
       "             'meglio': 614,\n",
       "             'mercato': 615,\n",
       "             'moderna': 616,\n",
       "             'moglie': 617,\n",
       "             'momento': 618,\n",
       "             'nasce': 619,\n",
       "             'nord': 620,\n",
       "             'obiettivo': 621,\n",
       "             'oggetti': 622,\n",
       "             'ossigeno': 623,\n",
       "             'percorso': 624,\n",
       "             'pianoforte': 625,\n",
       "             'poiché': 626,\n",
       "             'posizione': 627,\n",
       "             'presente': 628,\n",
       "             'principale': 629,\n",
       "             'qua': 630,\n",
       "             'qualsiasi': 631,\n",
       "             'quan': 632,\n",
       "             'quell': 633,\n",
       "             'rapporto': 634,\n",
       "             'regno': 635,\n",
       "             'rimase': 636,\n",
       "             'scopo': 637,\n",
       "             'secoli': 638,\n",
       "             'storie': 639,\n",
       "             'successivamente': 640,\n",
       "             'temi': 641,\n",
       "             'troppo': 642,\n",
       "             'usa': 643,\n",
       "             'utilizzato': 644,\n",
       "             'vari': 645,\n",
       "             'album': 646,\n",
       "             'alpe': 647,\n",
       "             'alto': 648,\n",
       "             'ambiente': 649,\n",
       "             'amici': 650,\n",
       "             'anch': 651,\n",
       "             'artisti': 652,\n",
       "             'autostrada': 653,\n",
       "             'avere': 654,\n",
       "             'cassio': 655,\n",
       "             'centrale': 656,\n",
       "             'compositore': 657,\n",
       "             'composizione': 658,\n",
       "             'costretto': 659,\n",
       "             'costruzione': 660,\n",
       "             'detta': 661,\n",
       "             'diede': 662,\n",
       "             'diritto': 663,\n",
       "             'donne': 664,\n",
       "             'elettrico': 665,\n",
       "             'esempi': 666,\n",
       "             'ex': 667,\n",
       "             'faustina': 668,\n",
       "             'figli': 669,\n",
       "             'figlia': 670,\n",
       "             'fuori': 671,\n",
       "             'giugno': 672,\n",
       "             'haydn': 673,\n",
       "             'idea': 674,\n",
       "             'magnetico': 675,\n",
       "             'merito': 676,\n",
       "             'mezzo': 677,\n",
       "             'milano': 678,\n",
       "             'militare': 679,\n",
       "             'natura': 680,\n",
       "             'naturale': 681,\n",
       "             'nedi': 682,\n",
       "             'notte': 683,\n",
       "             'occidentale': 684,\n",
       "             'ordine': 685,\n",
       "             'ottenne': 686,\n",
       "             'parole': 687,\n",
       "             'passato': 688,\n",
       "             'personale': 689,\n",
       "             'pertanto': 690,\n",
       "             'piloti': 691,\n",
       "             'portata': 692,\n",
       "             'presidente': 693,\n",
       "             'presto': 694,\n",
       "             'problemi': 695,\n",
       "             'qualche': 696,\n",
       "             'raggiungere': 697,\n",
       "             'rapporti': 698,\n",
       "             'reale': 699,\n",
       "             'relazione': 700,\n",
       "             'romano': 701,\n",
       "             'scrittura': 702,\n",
       "             'sette': 703,\n",
       "             'settembre': 704,\n",
       "             'specie': 705,\n",
       "             'suicidio': 706,\n",
       "             'tecnica': 707,\n",
       "             'territorio': 708,\n",
       "             'titoli': 709,\n",
       "             'todi': 710,\n",
       "             'tradizionale': 711,\n",
       "             'università': 712,\n",
       "             'vera': 713,\n",
       "             'vienna': 714,\n",
       "             'vivere': 715,\n",
       "             'volume': 716,\n",
       "             'andò': 717,\n",
       "             'appare': 718,\n",
       "             'armi': 719,\n",
       "             'arrivò': 720,\n",
       "             'arti': 721,\n",
       "             'ascesa': 722,\n",
       "             'attuale': 723,\n",
       "             'bambini': 724,\n",
       "             'bene': 725,\n",
       "             'berlino': 726,\n",
       "             'carta': 727,\n",
       "             'ceh': 728,\n",
       "             'cia': 729,\n",
       "             'colle': 730,\n",
       "             'colori': 731,\n",
       "             'commodo': 732,\n",
       "             'confronti': 733,\n",
       "             'cor': 734,\n",
       "             'descrive': 735,\n",
       "             'dice': 736,\n",
       "             'edl': 737,\n",
       "             'esercito': 738,\n",
       "             'est': 739,\n",
       "             'europea': 740,\n",
       "             'febbraio': 741,\n",
       "             'fonti': 742,\n",
       "             'for': 743,\n",
       "             'frontone': 744,\n",
       "             'giovanni': 745,\n",
       "             'immagini': 746,\n",
       "             'iniziò': 747,\n",
       "             'lasciò': 748,\n",
       "             'llaa': 749,\n",
       "             'luce': 750,\n",
       "             'maestro': 751,\n",
       "             'manga': 752,\n",
       "             'massimo': 753,\n",
       "             'meiji': 754,\n",
       "             'mesi': 755,\n",
       "             'moneta': 756,\n",
       "             'n': 757,\n",
       "             'nacque': 758,\n",
       "             'novembre': 759,\n",
       "             'nuove': 760,\n",
       "             'orientale': 761,\n",
       "             'ottenere': 762,\n",
       "             'otto': 763,\n",
       "             'pace': 764,\n",
       "             'parola': 765,\n",
       "             'piccolo': 766,\n",
       "             'possibilità': 767,\n",
       "             'potere': 768,\n",
       "             'precedente': 769,\n",
       "             'quarto': 770,\n",
       "             'rappresenta': 771,\n",
       "             'regole': 772,\n",
       "             'riguarda': 773,\n",
       "             'scrit': 774,\n",
       "             'sede': 775,\n",
       "             'sessanta': 776,\n",
       "             'sino': 777,\n",
       "             'soltanto': 778,\n",
       "             'stampa': 779,\n",
       "             'state': 780,\n",
       "             'stava': 781,\n",
       "             'storici': 782,\n",
       "             'strumenti': 783,\n",
       "             'studiosi': 784,\n",
       "             'successivo': 785,\n",
       "             'tanizaki': 786,\n",
       "             'team': 787,\n",
       "             'tela': 788,\n",
       "             'tempi': 789,\n",
       "             'tentativo': 790,\n",
       "             'terza': 791,\n",
       "             'tito': 792,\n",
       "             'traffico': 793,\n",
       "             'tut': 794,\n",
       "             'ufficiale': 795,\n",
       "             'ulteriori': 796,\n",
       "             'vanno': 797,\n",
       "             'zio': 798,\n",
       "             'zioni': 799,\n",
       "             'abitanti': 800,\n",
       "             'affari': 801,\n",
       "             'age': 802,\n",
       "             'amico': 803,\n",
       "             'apocrifo': 804,\n",
       "             'attenzione': 805,\n",
       "             'atto': 806,\n",
       "             'b': 807,\n",
       "             'banda': 808,\n",
       "             'brevi': 809,\n",
       "             'canone': 810,\n",
       "             'carattere': 811,\n",
       "             'case': 812,\n",
       "             'certo': 813,\n",
       "             'chiusa': 814,\n",
       "             'cinquanta': 815,\n",
       "             'citazioni': 816,\n",
       "             'classici': 817,\n",
       "             'clima': 818,\n",
       "             'com': 819,\n",
       "             'conte': 820,\n",
       "             'contenuti': 821,\n",
       "             'contesto': 822,\n",
       "             'costituito': 823,\n",
       "             'dati': 824,\n",
       "             'dela': 825,\n",
       "             'deriva': 826,\n",
       "             'difesa': 827,\n",
       "             'diffusione': 828,\n",
       "             'direttore': 829,\n",
       "             'donna': 830,\n",
       "             'duello': 831,\n",
       "             'elio': 832,\n",
       "             'entrambi': 833,\n",
       "             'fanno': 834,\n",
       "             'funzione': 835,\n",
       "             'giacomo': 836,\n",
       "             'golden': 837,\n",
       "             'governatore': 838,\n",
       "             'grotesque': 839,\n",
       "             'guida': 840,\n",
       "             'impiego': 841,\n",
       "             'incidente': 842,\n",
       "             'iniziale': 843,\n",
       "             'inizialmente': 844,\n",
       "             'intero': 845,\n",
       "             'io': 846,\n",
       "             'ipotesi': 847,\n",
       "             'ispirato': 848,\n",
       "             'km': 849,\n",
       "             'letto': 850,\n",
       "             'limite': 851,\n",
       "             'linea': 852,\n",
       "             'macchina': 853,\n",
       "             'maria': 854,\n",
       "             'materia': 855,\n",
       "             'mi': 856,\n",
       "             'minore': 857,\n",
       "             'monogatari': 858,\n",
       "             'motivi': 859,\n",
       "             'neve': 860,\n",
       "             'novecento': 861,\n",
       "             'nowiki': 862,\n",
       "             'orchestre': 863,\n",
       "             'originale': 864,\n",
       "             'ossia': 865,\n",
       "             'ovvero': 866,\n",
       "             'partenza': 867,\n",
       "             'passo': 868,\n",
       "             'personaggio': 869,\n",
       "             'pi': 870,\n",
       "             'pista': 871,\n",
       "             'piste': 872,\n",
       "             'poche': 873,\n",
       "             'poema': 874,\n",
       "             'poeta': 875,\n",
       "             'presenta': 876,\n",
       "             'prezzo': 877,\n",
       "             'problema': 878,\n",
       "             'pubblici': 879,\n",
       "             'punta': 880,\n",
       "             'record': 881,\n",
       "             'ricorda': 882,\n",
       "             'riferimento': 883,\n",
       "             'ritiro': 884,\n",
       "             'ritmo': 885,\n",
       "             'ritorno': 886,\n",
       "             'scelta': 887,\n",
       "             'sci': 888,\n",
       "             'sconto': 889,\n",
       "             'scrittori': 890,\n",
       "             'scrivere': 891,\n",
       "             'secon': 892,\n",
       "             'segno': 893,\n",
       "             'sociali': 894,\n",
       "             'soldati': 895,\n",
       "             'sole': 896,\n",
       "             'storico': 897,\n",
       "             'struttura': 898,\n",
       "             'teoria': 899,\n",
       "             'testo': 900,\n",
       "             'tibet': 901,\n",
       "             'toche': 902,\n",
       "             'tornò': 903,\n",
       "             'ufficiali': 904,\n",
       "             'unità': 905,\n",
       "             'uomini': 906,\n",
       "             'utilizza': 907,\n",
       "             'variazioni': 908,\n",
       "             'venti': 909,\n",
       "             'verrà': 910,\n",
       "             'villa': 911,\n",
       "             'von': 912,\n",
       "             'walser': 913,\n",
       "             'york': 914,\n",
       "             'zona': 915,\n",
       "             'alpinisti': 916,\n",
       "             'amministrazione': 917,\n",
       "             'anime': 918,\n",
       "             'arrivare': 919,\n",
       "             'assieme': 920,\n",
       "             'avevano': 921,\n",
       "             'avrà': 922,\n",
       "             'avuto': 923,\n",
       "             'banca': 924,\n",
       "             'basso': 925,\n",
       "             'biografo': 926,\n",
       "             'brani': 927,\n",
       "             'buon': 928,\n",
       "             'c': 929,\n",
       "             'capacità': 930,\n",
       "             'caratteri': 931,\n",
       "             'carlo': 932,\n",
       "             'celebre': 933,\n",
       "             'certa': 934,\n",
       "             'cesare': 935,\n",
       "             'chiese': 936,\n",
       "             'circuito': 937,\n",
       "             'classe': 938,\n",
       "             'colloqui': 939,\n",
       "             'commercio': 940,\n",
       "             'composto': 941,\n",
       "             'conto': 942,\n",
       "             'critici': 943,\n",
       "             'culturale': 944,\n",
       "             'decise': 945,\n",
       "             'dedicato': 946,\n",
       "             'die': 947,\n",
       "             'dietro': 948,\n",
       "             'difficile': 949,\n",
       "             'difficoltà': 950,\n",
       "             'direttamente': 951,\n",
       "             'diretto': 952,\n",
       "             'distanza': 953,\n",
       "             'don': 954,\n",
       "             'dovuto': 955,\n",
       "             'economiche': 956,\n",
       "             'elevato': 957,\n",
       "             'enl': 958,\n",
       "             'es': 959,\n",
       "             'esistono': 960,\n",
       "             'essendo': 961,\n",
       "             'evento': 962,\n",
       "             'flusso': 963,\n",
       "             'fondo': 964,\n",
       "             'formazione': 965,\n",
       "             'fratelli': 966,\n",
       "             'free': 967,\n",
       "             'generalmente': 968,\n",
       "             'giapponesi': 969,\n",
       "             'gio': 970,\n",
       "             'giri': 971,\n",
       "             'giunse': 972,\n",
       "             'hce': 973,\n",
       "             'he': 974,\n",
       "             'ideale': 975,\n",
       "             'informazioni': 976,\n",
       "             'inghilterra': 977,\n",
       "             'ita': 978,\n",
       "             'italiane': 979,\n",
       "             'kawabata': 980,\n",
       "             'latino': 981,\n",
       "             'letterari': 982,\n",
       "             'letterarie': 983,\n",
       "             'linee': 984,\n",
       "             'linguaggio': 985,\n",
       "             'livenza': 986,\n",
       "             'maniera': 987,\n",
       "             'mar': 988,\n",
       "             'massa': 989,\n",
       "             'media': 990,\n",
       "             'mente': 991,\n",
       "             'militari': 992,\n",
       "             'mio': 993,\n",
       "             'mishima': 994,\n",
       "             'modelli': 995,\n",
       "             'modello': 996,\n",
       "             'moderno': 997,\n",
       "             'morto': 998,\n",
       "             'nominale': 999,\n",
       "             ...})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data,valid_data,test_data), \\\n",
    "    batch_size = BATCH_SIZE, \\\n",
    "    device= device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim: int, \n",
    "                 emb_dim: int, \n",
    "                 enc_hid_dim: int, \n",
    "                 dec_hid_dim: int, \n",
    "                 dropout: float):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, \n",
    "                src: Tensor) -> Tuple[Tensor]:\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src sent len, batch size, emb dim]\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "                \n",
    "        #outputs = [src sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        # Note: torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        # is of shape [batch_size, enc_hid_dim * 2]\n",
    "        \n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        #outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 enc_hid_dim: int, \n",
    "                 dec_hid_dim: int,\n",
    "                 attn_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        \n",
    "        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n",
    "        \n",
    "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
    "        self.v = nn.Parameter(torch.rand(attn_dim))\n",
    "        \n",
    "    def forward(self, \n",
    "                decoder_hidden: Tensor, \n",
    "                encoder_outputs: Tensor) -> Tensor:\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #decoder_hidden = [batch size, src sent len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "        \n",
    "        # Step 1: to enable feeding through \"self.attn\" pink box above, concatenate \n",
    "        # `repeated_decoder_hidden` and `encoder_outputs`:\n",
    "        # torch.cat((hidden, encoder_outputs), dim = 2) has shape \n",
    "        # [batch_size, seq_len, enc_hid_dim * 2 + dec_hid_dim]\n",
    "        \n",
    "        # Step 2: feed through self.attn to end up with:\n",
    "        # [batch_size, seq_len, attn_dim]\n",
    "        \n",
    "        # Step 3: feed through tanh       \n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((\n",
    "            repeated_decoder_hidden, \n",
    "            encoder_outputs), \n",
    "            dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src sent len, attn_dim]\n",
    "        \n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        \n",
    "        #energy = [batch size, attn_dim, src sent len]\n",
    "        \n",
    "        #v = [attn_dim]\n",
    "        \n",
    "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
    "        \n",
    "        #v = [batch size, 1, attn_dim]\n",
    "        \n",
    "        # High level: energy a function of both encoder element outputs and most recent decoder hidden state,\n",
    "        # of shape attn_dim x enc_seq_len for each observation\n",
    "        # v, being 1 x attn_dim, transforms this into a vector of shape 1 x enc_seq_len for each observation\n",
    "        # Then, we take the softmax over these to get the output of the attention function\n",
    "\n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "        \n",
    "        #attention= [batch size, src len]\n",
    "        \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 enc_hid_dim: int, \n",
    "                 dec_hid_dim: int,\n",
    "                 attn_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        \n",
    "        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n",
    "        \n",
    "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
    "        \n",
    "    def forward(self, \n",
    "                decoder_hidden: Tensor, \n",
    "                encoder_outputs: Tensor) -> Tensor:\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #decoder_hidden = [batch size, src sent len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "        \n",
    "        # Step 1: to enable feeding through \"self.attn\" pink box above, concatenate \n",
    "        # `repeated_decoder_hidden` and `encoder_outputs`:\n",
    "        # torch.cat((hidden, encoder_outputs), dim = 2) has shape \n",
    "        # [batch_size, seq_len, enc_hid_dim * 2 + dec_hid_dim]\n",
    "        \n",
    "        # Step 2: feed through self.attn to end up with:\n",
    "        # [batch_size, seq_len, attn_dim]\n",
    "        \n",
    "        # Step 3: feed through tanh       \n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((\n",
    "            repeated_decoder_hidden, \n",
    "            encoder_outputs), \n",
    "            dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src sent len, attn_dim]\n",
    "\n",
    "        attention = torch.sum(energy, dim=2)\n",
    "        #attention= [batch size, src len]\n",
    "        \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 output_dim: int, \n",
    "                 emb_dim: int, \n",
    "                 enc_hid_dim: int, \n",
    "                 dec_hid_dim: int, \n",
    "                 dropout: int, \n",
    "                 attention: nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "        # Note: from Attention: self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, attn_dim)\n",
    "        \n",
    "        # Note: `output_dim` same as `vocab_size`\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        \n",
    "        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def _weighted_encoder_rep(self, \n",
    "                              decoder_hidden: Tensor,\n",
    "                              encoder_outputs: Tensor) -> Tensor:\n",
    "        \n",
    "        # Attention, at a high level, takes in:\n",
    "        # The decoder hidden state\n",
    "        # All the \"seq_len\" encoder outputs\n",
    "        # Outputs a vector summing to 1 of length seq_len for each observation\n",
    "        a = self.attention(decoder_hidden, encoder_outputs)\n",
    "\n",
    "        #a = [batch size, src len]\n",
    "\n",
    "        a = a.unsqueeze(1)\n",
    "\n",
    "        #a = [batch size, 1, src len]\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
    "\n",
    "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
    "\n",
    "        #weighted_encoder_rep = [batch size, 1, enc hid dim * 2]\n",
    "\n",
    "        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n",
    "\n",
    "        #weighted_encoder_rep = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        return weighted_encoder_rep\n",
    "        \n",
    "        \n",
    "    def forward(self, \n",
    "                input: Tensor, \n",
    "                decoder_hidden: Tensor, \n",
    "                encoder_outputs: Tensor) -> Tuple[Tensor]:\n",
    "             \n",
    "        #input = [batch size] Note: \"one character at a time\"\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden, \n",
    "                                                          encoder_outputs)\n",
    "        \n",
    "        # Then, the input to the decoder _for this character_ is a concatenation of:\n",
    "        # This weighted attention\n",
    "        # The embedding itself\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim = 2)\n",
    "        \n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
    "        \n",
    "        #output = [sent len, batch size, dec hid dim * n directions]\n",
    "        #decoder_hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #sent len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == decoder_hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n",
    "        \n",
    "        output = self.out(torch.cat((output, \n",
    "                                     weighted_encoder_rep, \n",
    "                                     embedded), dim = 1))\n",
    "        \n",
    "        #output = [bsz, output dim]\n",
    "        \n",
    "        return output, decoder_hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, \n",
    "                 encoder: nn.Module, \n",
    "                 decoder: nn.Module, \n",
    "                 device: torch.device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, \n",
    "                src: Tensor, \n",
    "                trg: Tensor, \n",
    "                teacher_forcing_ratio: float = 0.5) -> Tensor:\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        #trg = [trg sent len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        \n",
    "        batch_size = src.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        output = trg[0,:]\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            output = (trg[t] if teacher_force else top1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ATTN_DIM = 64\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_attn = ModifiedAttention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
    "\n",
    "dec_mod_attn = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, mod_attn)\n",
    "\n",
    "model_mod_attn = Seq2Seq(enc, dec_mod_attn, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(5979, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): ModifiedAttention(\n",
       "      (attn): Linear(in_features=1536, out_features=64, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(6179, 256)\n",
       "    (rnn): GRU(1280, 512)\n",
       "    (out): Linear(in_features=1792, out_features=6179, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)\n",
    "model_mod_attn.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 19,935,651 trainable parameters\n",
      "The model has 19,935,587 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "print(f'The model has {count_parameters(model_mod_attn):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "optimizer_mod_attn = optim.Adam(model_mod_attn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, \n",
    "          iterator: BucketIterator, \n",
    "          optimizer: optim.Adam, \n",
    "          criterion: nn.modules.loss.CrossEntropyLoss, \n",
    "          clip: float):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg sent len, batch size]\n",
    "        #output = [trg sent len, batch size, output dim]\n",
    "        \n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg sent len - 1) * batch size]\n",
    "        #output = [(trg sent len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, \n",
    "             iterator: BucketIterator, \n",
    "             criterion: nn.modules.loss.CrossEntropyLoss):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg sent len, batch size]\n",
    "            #output = [trg sent len, batch size, output dim]\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg sent len - 1) * batch size]\n",
    "            #output = [(trg sent len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time: int, \n",
    "               end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'tut3-model.pt'\n",
    "MODEL_PATH_MOD_ATTN = 'tut3-model-modified_attn.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 12s\n",
      "\tTrain Loss: 7.195 | Train PPL: 1333.066\n",
      "\t Val. Loss: 6.028 |  Val. PPL: 414.790\n",
      "Epoch: 02 | Time: 0m 12s\n",
      "\tTrain Loss: 6.417 | Train PPL: 612.122\n",
      "\t Val. Loss: 5.928 |  Val. PPL: 375.221\n",
      "Epoch: 03 | Time: 0m 12s\n",
      "\tTrain Loss: 6.289 | Train PPL: 538.410\n",
      "\t Val. Loss: 5.967 |  Val. PPL: 390.212\n",
      "Epoch: 04 | Time: 0m 12s\n",
      "\tTrain Loss: 6.130 | Train PPL: 459.526\n",
      "\t Val. Loss: 5.874 |  Val. PPL: 355.824\n",
      "Epoch: 05 | Time: 0m 13s\n",
      "\tTrain Loss: 6.022 | Train PPL: 412.314\n",
      "\t Val. Loss: 5.877 |  Val. PPL: 356.563\n",
      "Epoch: 06 | Time: 0m 13s\n",
      "\tTrain Loss: 5.901 | Train PPL: 365.284\n",
      "\t Val. Loss: 5.896 |  Val. PPL: 363.458\n",
      "Epoch: 07 | Time: 0m 13s\n",
      "\tTrain Loss: 5.789 | Train PPL: 326.632\n",
      "\t Val. Loss: 5.947 |  Val. PPL: 382.624\n",
      "Epoch: 08 | Time: 0m 12s\n",
      "\tTrain Loss: 5.691 | Train PPL: 296.279\n",
      "\t Val. Loss: 5.958 |  Val. PPL: 387.014\n",
      "Epoch: 09 | Time: 0m 12s\n",
      "\tTrain Loss: 5.601 | Train PPL: 270.697\n",
      "\t Val. Loss: 5.931 |  Val. PPL: 376.467\n",
      "Epoch: 10 | Time: 0m 12s\n",
      "\tTrain Loss: 5.517 | Train PPL: 249.006\n",
      "\t Val. Loss: 5.934 |  Val. PPL: 377.637\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model_mod_attn, train_iterator, optimizer_mod_attn, criterion, CLIP)\n",
    "    valid_loss = evaluate(model_mod_attn, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_mod_attn.state_dict(), MODEL_PATH_MOD_ATTN)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 5.845 | Test PPL: 345.575 |\n"
     ]
    }
   ],
   "source": [
    "model_mod_attn.load_state_dict(torch.load(MODEL_PATH_MOD_ATTN))\n",
    "\n",
    "test_loss = evaluate(model_mod_attn, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 12s\n",
      "\tTrain Loss: 6.900 | Train PPL: 992.536\n",
      "\t Val. Loss: 6.002 |  Val. PPL: 404.248\n",
      "Epoch: 02 | Time: 0m 12s\n",
      "\tTrain Loss: 6.237 | Train PPL: 511.362\n",
      "\t Val. Loss: 5.851 |  Val. PPL: 347.665\n",
      "Epoch: 03 | Time: 0m 12s\n",
      "\tTrain Loss: 6.076 | Train PPL: 435.426\n",
      "\t Val. Loss: 5.854 |  Val. PPL: 348.612\n",
      "Epoch: 04 | Time: 0m 12s\n",
      "\tTrain Loss: 5.895 | Train PPL: 363.329\n",
      "\t Val. Loss: 5.872 |  Val. PPL: 354.919\n",
      "Epoch: 05 | Time: 0m 12s\n",
      "\tTrain Loss: 5.742 | Train PPL: 311.661\n",
      "\t Val. Loss: 5.949 |  Val. PPL: 383.499\n",
      "Epoch: 06 | Time: 0m 12s\n",
      "\tTrain Loss: 5.621 | Train PPL: 276.224\n",
      "\t Val. Loss: 5.975 |  Val. PPL: 393.430\n",
      "Epoch: 07 | Time: 0m 12s\n",
      "\tTrain Loss: 5.495 | Train PPL: 243.468\n",
      "\t Val. Loss: 6.039 |  Val. PPL: 419.428\n",
      "Epoch: 08 | Time: 0m 13s\n",
      "\tTrain Loss: 5.408 | Train PPL: 223.125\n",
      "\t Val. Loss: 5.963 |  Val. PPL: 388.761\n",
      "Epoch: 09 | Time: 0m 12s\n",
      "\tTrain Loss: 5.285 | Train PPL: 197.408\n",
      "\t Val. Loss: 5.974 |  Val. PPL: 393.264\n",
      "Epoch: 10 | Time: 0m 13s\n",
      "\tTrain Loss: 5.143 | Train PPL: 171.217\n",
      "\t Val. Loss: 6.028 |  Val. PPL: 414.720\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 5.817 | Test PPL: 335.975 |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
